\section{Introduction}
\label{section:IntroductionChapter}
Deep learning, was once considered a marginal field, while now it has become a key player in the rapidly evolving landscape of Artificial Intelligence (AI). In the past decade, the growth of deep learning has led to the development of innovative architectures and methods that enable machines to learn in ways previously thought impossible. From traditional supervised learning to more advanced forms such as unsupervised and self-supervised learning, to the innovative techniques of adversarial and reinforcement learning, AI has come a long way. Among these forms of learning, generative modelling has emerged as a major area of focus and has seen substantial growth. Thanks to deep learning, there is now a better understanding of what it means for an AI to learn and how to implement models that can self-learn. In particular, the advent of adversarial learning has played a major role in fuelling the AI explosion, since it has a large set of applications in generative modelling. Generative modelling refers to implementing AI models that can generate new and varied content through a variety of learning methods. With the ability to generate new content, AI is changing the way the world is experienced and understood. In the context of generative modelling, the term “generate” is crucial. When constructing a generative model, the focus is on creating new content, not simply modifying or filtering what already exists. This content can be generated randomly or, in certain cases, it can be generated based on certain conditions that are set by the person designing the model. The ability to conditionally control content generation allows for a greater degree of flexibility and precision in generating content that meets specific criteria~\cite{GeneratingNewRealityBook}.\\
The ultimate goal of generative modelling is to create new, unique content that was not present in the original dataset. Generative modelling has numerous applications, including image synthesis, data augmentation, and music generation, among others. Advances in generative modelling are constantly being made, and new techniques are being developed, making it a rapidly growing field of research with numerous opportunities for new discoveries and breakthroughs.

\subsection{Objective of the thesis and research question}
Generative Adversarial Networks have gained significant attention in recent years due to their ability to generate realistic images. However, it is challenging to evaluate the quality of image generation in these networks. This thesis aims to explore the extent to which generative adversarial networks can produce conditioned images. The investigation is guided by the desired to comprehend the abilities of these networks.

\noindent The thesis primary goal is to determine the level at which people can differentiate between images generated from sketches and real images that resemble the sketches. The research is not centred on developing a new neural network, but rather on evaluating the images produced by existing networks. The pre-trained StyleGAN network was utilised in this research, and the weights were employed to train a scheme called \textit{ReStyle} over a variation of the \textit{pixel2style2pixel} encoder on a custom dataset. \\
The research question that this thesis wants to answer is the following:
\begin{center}
    \textit{Can people accurately distinguish between a conditionally generated photorealistic image and real images of similar appearance?}
\end{center}

\noindent To assess the quality of the generated images, a survey was designed where participants were shown a sketch of a face and asked to identify which of the four additional images inspired the sketch. The results of the survey were analysed to determine the human ability to recognise generated images from authentic ones.
The images were generated based on sketches created by users on a tablet device, and they are very similar to real people's faces, thus making it challenging for humans to differentiate between authentic and fake images.

\subsection{Possible applications}
There are numerous potential applications for the generation of conditional images based on sketches, as the full extent of this technology's capabilities has yet to be explored. The following illustrates examples of the possible uses. For example, the generation of conditional images based on sketches can be used in the entertainment industry, where the technology could be used to create digital characters for video games, movies, and animations. But also the creation of custom avatars for social media and virtual reality.
\\
While one of the most impactful implementation of this research would be a sketch-based image retrieval system in criminal investigations. The technology could be useful for law enforcement, allowing forensic artists to create photorealistic renderings of suspects based on eyewitness sketches. \\
Unfortunately, not all the possible applications are “safe”, since there are some that could be potentially dangerous in case of misused or if applied without considering ethical and moral implications. Indeed, this technology could be used for the creation of fake identities.

\subsection{Recently emerged technology}
While conducting this research, similar techniques have emerged, including Stable Diffusion. Stable Diffusion is a text-to-image model that allows users to easily produce high-quality artwork in a few seconds based on a text prompt. Traditionally, the Stable Diffusion model is trained to generate images based on text inputs. However, thanks to a diffusion-denoising mechanism proposed by~\cite{SDEedit} the model can also be used for task such as text-guided image-to-image translation and upscaling.
%However, in its new version, the original text encoder has been removed and replaced with the CLIP (Contrastive Language-Image Pretraining) image encoder. As a result, instead of generating images based on text inputs, the model generates images that match CLIP's image embedding, resulting in images with similar style and content but with distinct details, particularly in terms of composition. CLIP \cite{CLIP} is a deep learning model developed by OpenAI that combines computer vision and natural language processing to learn a universal representation of images and texts. 
The approach offer a similar solution to the problem that this study was investigating, but ultimately, the research was carried on. \\
The results obtained from Stable Diffusion are remarkable due to its extensive training on a massive dataset and on highly powerful GPUs. The long training time allowed the network to learn intricate patterns and relationships in the data, resulting in high-quality outputs. The use of powerful GPUs also ensured a faster training process, allowing the network to converge quickly to a satisfactory solution.

\subsection{Structure of the thesis}
This section describes the thesis's structure. Chapter~\ref{section:IntroductionChapter} provides a comprehensive overview of artificial intelligence and generative models. It also includes the objective and guiding research question of the thesis, as well as several potential applications of the project.\\
Chapter~\ref{section:literatureReviewChapter} of the thesis is dedicated to a comprehensive review of the existing literature on generative adversarial networks. In this chapter, the reader will gain an understanding of the background and evolution of these networks.\\
In chapter~\ref{section:datasetChapter} there will be a detailed discussion on the creation of the dataset, including the selection criteria, data pre-processing and preparation processes. This chapter will dig deeper into the methods used to build the dataset, the reasons for selecting the specific data and how it was processed. Additionally, it explores the theories behind the technologies used to process the images.\\
Chapter~\ref{sec:used GAN discussion} will also encompass a detailed discussion of the specific generative adversarial networks that are used in the project, including their architecture and training methodologies.\\
In chapter~\ref{sec:training and testing setup} will be cover a discussion about the training and testing setup.\\
Chapter~\ref{sec:evaluation} is dedicated to evaluation of the generated images.\\
Finally, chapter~\ref{sec:conclusion} ...\\
